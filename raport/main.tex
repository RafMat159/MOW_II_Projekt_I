\documentclass[10pt,letterpaper]{article}

\usepackage[utf8]{inputenc}
%\usepackage[polish]{babel}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{placeins}
\PassOptionsToPackage{hyphens}{url}
\usepackage{listings}
\usepackage{color}
\usepackage{pdflscape}
\usepackage{adjustbox}
\lstloadlanguages{TeX}
\lstset{
	literate={ą}{{\k{a}}}1
           {ć}{{\'c}}1
           {ę}{{\k{e}}}1
           {ó}{{\'o}}1
           {ń}{{\'n}}1
           {ł}{{\l{}}}1
           {ś}{{\'s}}1
           {ź}{{\'z}}1
           {ż}{{\.z}}1
           {Ą}{{\k{A}}}1
           {Ć}{{\'C}}1
           {Ę}{{\k{E}}}1
           {Ó}{{\'O}}1
           {Ń}{{\'N}}1
           {Ł}{{\L{}}}1
           {Ś}{{\'S}}1
           {Ź}{{\'Z}}1
           {Ż}{{\.Z}}1,
	basicstyle=\footnotesize\ttfamily,
}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[left=1.5cm, right=1.5cm, top=2cm, bottom=2cm]{geometry}
\usepackage{longtable}


% Include other packages here, before hyperref.

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Analiza i prognozowanie cen samochodów używanych na aukcjach z wykorzystaniem regresji}

\author{Zuzanna Jużyniec\\
{\tt\small zuzanna.juzyniec@student.pk.edu.pl}
\and
Rafał Matusiak\\
{\tt\small rafal.matusiak@student.pk.edu.pl}
}
\maketitle
%\thispagestyle{empty}

\section{Abstract}
\section{Wprowadzenie}

\newpage
\section{Zbiór danych}
Zbiór danych został pobrany ze strony \url{https://www.kaggle.com/datasets/tunguz/used-car-auction-prices?resource=download} i zawiera historyczne ceny sprzedaży samochodów na aukcjach, zebrane z zewnętrznych źródeł internetowych. Został zebrany w 2015 roku i nie był już aktualizowany.

Zbiór danych zawiera następujące kolumny:
\begin{itemize}
\item year - rok produkcji pojazdu;
\item make - marka pojazdu (np. Toyota, Ford);
\item model - model pojazdu (np. Corolla, Mustang);
\item trim - wersja wyposażenia pojazdu (np. SE, Base);
\item body - rodzaj nadwozia pojazdu (np. sedan, SUV);
\item transmission - rodzaj skrzyni biegów (np. automatyczna, manualna);
\item vin - numer identyfikacyjny pojazdu (VIN);
\item state - stan (lokalizacja geograficzna) w USA, gdzie odbyła się sprzedaż;
\item condition - stan pojazdu oceniany w skali od 1 do 5, gdzie wyższe wartości oznaczają lepszy \item stan techniczny (zmienna ciągła);
\item odometer - przebieg pojazdu w milach;
\item color - kolor karoserii pojazdu;
\item interior - kolor wnętrza pojazdu;
\item seller - nazwa lub identyfikator sprzedającego pojazd;
\item mmr - wartość pojazdu według Manheim Market Report (MMR), używana jako wskaźnik wartości rynkowej;
\item sellingprice - rzeczywista cena sprzedaży pojazdu na aukcji;
\item saledate - data sprzedaży pojazdu.
\end{itemize}

W celu poznania danych zostało wykonane wiele wykresów.
\subsection{Wykres procentowej ilości brakujących wartości}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/brakujace.png}
\caption{Histogramy zmiennych numerycznych.}
   \label{fig:other-figure}
\end{center}
\end{figure}
Jak widać wartości NaN występują w 9 kolumnach, najwięcej w zmiennej 'transmission'.

\subsection{Histogramy}

Jak widać na Rys. 1, niektóre zmienne, takie jak sellingprice, mmr oraz odometer, charakteryzują się rozkładem długoogonowym, co oznacza, że większość wartości koncentruje się w niższych przedziałach, jednak sporadycznie występują bardzo wysokie wartości odstające. Taki rozkład sugeruje, że typowe ceny sprzedaży, wartości rynkowe oraz przebiegi są zazwyczaj niskie, choć zdarzają się pojazdy o wyjątkowo wysokich cenach i dużym przebiegu. 

Aby lepiej zobrazować te rozkłady, zastosowano przekształcenie logarytmiczne, co przedstawiono na Rys. 2 — po tej transformacji rozkłady stały się bardziej symetryczne i zbliżone do normalnego, co ułatwiło ich interpretację. Zmienna condition ma bardziej równomierny rozkład, wskazując na stabilny stan techniczny większości pojazdów, natomiast year wykazuje tendencję przesunięcia ku nowszym rocznikom, co sugeruje, że na aukcjach dominują młodsze samochody. Podczas dalszej analizy wyniknie, czy takie przekształcenie będzie konieczne w procesie modelowania, aby poprawić wyniki prognozowania, jednak na tym etapie pozostajemy przy badaniu oryginalnych danych.

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/histogramy.png}
\caption{Histogramy zmiennych numerycznych.}
   \label{fig:other-figure}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/histogramy_log.png}
   \caption{Histogramy zmiennych numerycznych po przekształceniu logarytmicznym.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}

\subsection{Boxploty}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/boxplot_wszystko.png}
   \caption{Boxploty dla wszystkich zmiennych numerycznych.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Na powyższym wykresie (Rys. 4) przedstawiono boxploty dla zmiennych numerycznych, jednak ze względu na dużą różnicę w wartościach pomiędzy zmiennymi, szczegóły niektórych cech są trudne do dostrzeżenia. Aby lepiej zobrazować rozkład wartości, w tym wartości odstające, zostaną przedstawione oddzielne boxploty dla każdej z cech numerycznych. Dodatkowo, wykresy te zostaną wygenerowane zarówno z uwzględnieniem danych odstających, jak i bez nich, aby zapewnić pełniejszą analizę danych.

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/boxplot_sellingprice.png}
   \caption{Boxploty dla zmiennej 'sellingprice'.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/boxplot_odometer.png}
   \caption{Boxploty dla zmiennej 'odometer'.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/boxplot_mmr.png}
   \caption{Boxploty dla zmiennej 'mmr'.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/boxplot_condition.png}
   \caption{Boxploty dla zmiennej 'condition'.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/boxplot_year.png}
   \caption{Boxploty dla zmiennej 'year'.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}

Wartości odstające mają największy wpływ na zmienne takie jak cena sprzedaży (sellingprice), przebieg (odometer) i wartość rynkowa (mmr), które charakteryzują się nielicznymi, ale bardzo skrajnymi wartościami odstającymi. Po ich usunięciu uzyskujemy bardziej klarowny obraz typowego rozkładu danych, co pozwala lepiej zrozumieć, jakie są rzeczywiste wartości dominujące w zbiorze danych.

W przypadku zmiennych rok produkcji (year) i stan techniczny (condition), wartości odstające mają mniejszy wpływ. Boxplot dla stanu technicznego jest bardzo jednorodny, co wskazuje, że ta zmienna nie zawiera wielu ekstremalnych wartości, a typowy stan techniczny pojazdów waha się od 2.5 do 4.5.

\subsection{Pairplot}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/pairplot.png}
   \caption{Pairplot.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Patrząc na pairplot, widzimy wyraźne zależności między niektórymi zmiennymi (np. cena sprzedaży a rok produkcji, przebieg czy wartość rynkowa). Aby lepiej zrozumieć, jak silne są te zależności, warto obliczyć współczynniki korelacji dla tych zmiennych. Pozwoli to na dokładniejszą ocenę siły tych relacji.

\subsection{Macierz korelacji}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/korelacja.png}
   \caption{Macierz korelacji.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Rok produkcji (year) ma silną dodatnią korelację z ceną sprzedaży (0.59), co oznacza, że nowsze samochody zazwyczaj sprzedają się za wyższą cenę. Stan techniczny (condition) ma dodatnią korelację z ceną sprzedaży (0.54), co sugeruje, że samochody w lepszym stanie są droższe. Przebieg (odometer) wykazuje silną ujemną korelację z ceną sprzedaży (-0.58), co oznacza, że większy przebieg zmniejsza wartość samochodu. MMR (wartość rynkowa) ma bardzo silną dodatnią korelację z ceną sprzedaży (0.98), co wskazuje, że wartość rynkowa samochodu dobrze prognozuje jego cenę na aukcji.

\subsection{Wykres sprzedaży samochodów - Cena, Przebieg i Stan pojazdu dla 1000 losowych próbek}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/sprzedaz_samochodow.png}
   \caption{Wykres sprzedaży samochodów.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Na powyższym wykresie zbadano zależność między przebiegiem pojazdów a ceną sprzedaży, uwzględniając jednocześnie ich stan techniczny. Zauważalna jest silna ujemna zależność – samochody z większym przebiegiem są zazwyczaj tańsze. Lepszy stan techniczny pojazdu wpływa pozytywnie na cenę sprzedaży, co pozwala niektórym pojazdom osiągnąć wyższe ceny, nawet przy relatywnie dużym przebiegu. Najwyższe ceny uzyskują samochody w najlepszym stanie, podczas gdy pojazdy w najgorszym stanie sprzedają się za niższe kwoty, niezależnie od przebiegu.

\subsection{Wykres sprzedaży samochodów - Cena, Przebieg i Rok produkcji pojazdu dla 1000 losowych próbek}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/sprzedaz_samochodow_cena_przebieg_rok.png}
   \caption{Wykres sprzedaży samochodów.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Wykres przedstawia zależność między rokiem produkcji a ceną sprzedaży samochodów, uwzględniając jednocześnie ich przebieg. Zauważalna jest wyraźna tendencja – nowsze samochody (z lat 2010–2015) osiągają wyższe ceny, podczas gdy starsze pojazdy (sprzed 2005 roku) sprzedają się za znacznie niższe kwoty. Przebieg również ma istotny wpływ – samochody z mniejszym przebiegiem mają tendencję do wyższych cen, zwłaszcza w przypadku nowszych roczników. Samochody z większym przebiegiem, niezależnie od roku produkcji, zazwyczaj mają niższą wartość.

\subsection{Liczba sprzedanych pojazdów w poszczególnych stanach USA}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/liczba_sprzedanych_pojazdow_w_poszcegolnych_stanach.png}
   \caption{Liczba sprzedanych pojazdów w poszczególnych stanach USA.}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Największą liczbę sprzedanych pojazdów odnotowano w stanach Floryda (FL) oraz Kalifornia (CA). Te dwa stany zdecydowanie wyróżnią się jako liderzy. W czołówce są także Teksas (TX), Pensylwania (PA) oraz Georgia (GA).

\subsection{Top 10 najczęściej sprzedawanych marek samochodów}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/liczba_sprzedanych_pojazdow_marka.png}
   \caption{Top 10 najczęściej sprzedawanych marek samochodów}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Ford zdecydowanie dominuje jako najczęściej sprzedawana marka, z wyraźną przewagą nad Chevroletem oraz Nissanem. Toyota oraz Dodge zamykają czołową piątkę. Wynik ten może odzwierciedlać popularność tych marek na rynku amerykańskim, gdzie dominują pojazdy amerykańskie i azjatyckie.

\subsection{Liczba sprzedanych pojazdów według roku produkcji}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/liczba_sprzedanych_pojazdow_rok.png}
   \caption{Liczba sprzedanych pojazdów według roku produkcji}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Największa liczba sprzedanych pojazdów pochodzi z lat 2011-2014, co sugeruje, że rynek używanych samochodów obejmuje głównie nowsze modele, które mają jeszcze sporo lat eksploatacji przed sobą. Starsze roczniki, szczególnie te sprzed 2000 roku, są rzadziej spotykane, co jest zgodne z trendem zmniejszania się podaży starszych aut.

\subsection{Top 10 najlepiej sprzedających się typów nadwozia}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/typ_nadwozia.png}
   \caption{Top 10 najlepiej sprzedających się typów nadwozia}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Sedan jest zdecydowanie najpopularniejszym typem nadwozia, z liczbą sprzedanych egzemplarzy wyraźnie przewyższającą inne typy. SUV zajmuje drugie miejsce, co świadczy o rosnącej popularności pojazdów tego segmentu. Inne typy nadwozia, takie jak hatchback czy minivan, również znajdują się w czołówce, ale z mniejszą liczbą sprzedanych egzemplarzy.

\subsection{Top 10 kolorów z największą liczbą sprzedanych pojazdów}
\begin{figure}[H]
\begin{center}
   \includegraphics[width=0.7\linewidth]{figures/top_kolor_sprzedaz.png}
   \caption{Top 10 kolorów z największą liczbą sprzedanych pojazdów}
   \label{fig:other-figure-log}
\end{center}
\end{figure}
Kolory takie jak czarny, biały i srebrny są najczęściej wybierane przez nabywców, co nie jest zaskoczeniem, biorąc pod uwagę ich popularność na rynku. Są to kolory neutralne, które zazwyczaj lepiej utrzymują swoją wartość na rynku wtórnym. Inne kolory, takie jak niebieski i czerwony, również znajdują się w top 10, ale w mniejszych ilościach.

\section{Przegląd literatury}
Zbiór nie był wykorzystywany w publikacjach. Dostepne jest kilka notatników z przykładowymi podejścami do problemu. 
\begin{itemize}
    \item Notatnik Kaggle autorstwa Zabihullaha18 dotyczy predykcji cen samochodów za pomocą uczenia maszynowego. Zawiera kroki takie jak wstępne przetwarzanie danych, wybór cech i trenowanie modeli. Używa algorytmu regresji liniowej do przewidywania cen na podstawie danych o samochodach (np. marka, model, przebieg). Wyniki są oceniane przy użyciu wskaźników takich jak błąd średniokwadratowy, który pozwala mierzyć dokładność prognoz. Pelen notatnik dostępny jest pod adresem: \url{https://www.kaggle.com/code/zabihullah18/car-price-prediction#--5)-Train-the-Model-}
    \item Notatnik na Kaggle dotyczący predykcji cen używanych samochodów autorstwa Midoel3ila wykorzystuje techniki uczenia maszynowego do przewidywania cen samochodów na podstawie ich cech. Proces obejmuje wstępne przetwarzanie danych, eksplorację cech takich jak marka, model, przebieg i wiek pojazdu, a następnie budowanie modeli predykcyjnych. Algorytmy takie jak regresja liniowa i lasy losowe są stosowane do trenowania modelu. Notatnik ocenia dokładność przewidywań za pomocą miar takich jak błąd średniokwadratowy. Pelen notatnik dostępny jest pod adresem: \url{https://www.kaggle.com/code/midoel3ila/car-used-priced-prediction-using-ml}  
\end{itemize}

\section{Motywacja}
Projekt dotyczący prognozowania cen samochodów używanych jest interesujący, ponieważ może przynieść korzyści zarówno kupującym, jak i sprzedającym, pomagając przewidzieć realistyczne ceny rynkowe. W społeczeństwie, gdzie kupno używanych pojazdów jest powszechne, dokładne modele predykcyjne mogą wspierać transparentność i zaufanie w transakcjach. W przemyśle, takie prognozy mogą być użyteczne dla wypożyczalni samochodów, dealerów oraz platform aukcyjnych, pomagając w lepszym oszacowaniu wartości pojazdów i optymalizacji strategii sprzedaży.

\section{Ewaluacja}
Oczekiwany model będzie w stanie dokładnie przewidzieć ceny samochodów używanych na aukcjach na podstawie dostępnych danych. Satysfakcjonujący wynik to taki, w którym błąd predykcji (np. błąd średniokwadratowy) będzie niski, co oznacza, że przewidywane ceny są bliskie rzeczywistym cenom sprzedaży. Osiągniety wynik, powinien być wystarczająco precyzyjny, aby umożliwić zastosowanie modelu w rzeczywistych warunkach — np. na platformach aukcyjnych lub w firmach dealerskich — gdzie może pomóc w wycenie pojazdów. Oczywiście projekt ma charakter wyłącznie edukacyjny i nie jest przeznaczony do komercyjnego wykorzystania ani sprzedaży. Celem jest nauka i zrozumienie metod prognozowania cen samochodów używanych z wykorzystaniem modeli uczenia maszynowego. Chociaż dążymy do uzyskania dokładnych wyników, system ten nie będzie wdrażany w żadnym rzeczywistym środowisku komercyjnym. Skupiamy się na poszerzaniu wiedzy i umiejętności w zakresie analizy danych oraz budowania modeli predykcyjnych.

\section{Zasoby}
W projekcie zostanie wykorzystany język programowania Python oraz środowisko Jupyter Notebook do analizy danych i budowy modeli predykcyjnych. Python oferuje bogaty zestaw bibliotek do uczenia maszynowego, takich jak scikit-learn do budowy i oceny modeli, pandas do manipulacji danymi, oraz numpy do obliczeń numerycznych. Dodatkowo, zosatnie wykorzystana biblioteka matplotlib i seaborn do wizualizacji danych, co pozwoli na lepsze zrozumienie zależności i wzorców w zebranych informacjach.

\section{Zastosowane metody}
\section{Eksperyment}
\subsection{Preprocessing}

W celu przygotowania danych do modelowania i analizy predykcyjnej, przeprowadzono zaawansowany preprocessing, który obejmował różne etapy mające na celu poprawienie jakości i spójności zbioru danych. Szczegółowe kroki opisano poniżej.

\paragraph{1. Obliczenie wieku samochodu (\texttt{car\_age})}
Wiek samochodu w momencie sprzedaży został obliczony na podstawie kolumny \texttt{saledate} poprzez odjęcie od roku sprzedaży (\texttt{sale\_year}) roku produkcji pojazdu (\texttt{year}). Dodatkowo, z tej samej kolumny (\texttt{saledate}) wyodrębniono dodatkowe cechy, takie jak:
\begin{itemize}
    \item \textbf{Miesiąc sprzedaży} (\texttt{sale\_month}),
    \item \textbf{Rok sprzedaży} (\texttt{sale\_year}),
    \item \textbf{Godzina sprzedaży} (\texttt{sale\_hour}),
    \item \textbf{Minuta sprzedaży} (\texttt{sale\_minute}).
\end{itemize}

Po tej transformacji przeanalizowano sprzedaż samochodów w różnych miesiącach i dniach tygodnia. Wyniki wskazują, że najwięcej sprzedaży miało miejsce w styczniu (140,815 samochodów) oraz lutym (163,054 samochody). Z kolei analiza sprzedaży według dnia tygodnia wykazała, że transakcje najczęściej odbywają się na początku tygodnia, zwłaszcza we wtorki (180,158 sprzedaży), natomiast najmniej w niedziele (11,868 sprzedaży).

Podczas analizy wieku pojazdów wykryto, że 201 rekordów miało ujemne wartości w kolumnie \texttt{car\_age}. Można zatem założyć, że taka liczba samochodów została zakupiona w przedsprzedaży.

\paragraph{2. Ekstrakcja informacji z numeru VIN}
Z numeru VIN pojazdu przy użyciu biblioteki \texttt{vininfo} wyodrębniono informacje o kraju produkcji (\texttt{vin\_country}). Na tym etapie zrezygnowano z pozyskiwania dodatkowych danych, takich jak typ pojazdu, ponieważ były one już zawarte w innych kolumnach (\texttt{model}, \texttt{body}). Po wyodrębnieniu kraju, kolumna \texttt{vin} została usunięta, gdyż nie była już potrzebna do dalszej analizy.

\paragraph{3. Usunięcie wartości odstających}
W celu usunięcia wartości odstających przeprowadzono analizę cech numerycznych (\texttt{year}, \texttt{condition}, \texttt{odometer}, \texttt{mmr}, \texttt{sellingprice}) za pomocą metody IQR (Interquartile Range). Wartości spoza zakresu $[Q1 - 1.5 * IQR, Q3 + 1.5 * IQR]$ zostały usunięte. Łącznie usunięto około 8.3\% danych, co było akceptowalne, ponieważ nie przekraczało 10\% zbioru.

\paragraph{4. Przekształcenie zmiennych kategorycznych na wartości liczbowe}
Wszystkie zmienne kategoryczne (\texttt{make}, \texttt{model}, \texttt{trim}, \texttt{body}, \texttt{transmission}, \texttt{state}, \texttt{color}, \texttt{interior}, \texttt{seller}, \texttt{vin\_country}) zostały przekonwertowane na wartości liczbowe. Dla kolumny \texttt{transmission} zastosowano kodowanie binarne: \texttt{0} dla manualnej skrzyni biegów oraz \texttt{1} dla automatycznej. Dla pozostałych kolumn zastosowano "label encoding" co w kolejnych etapach może ograniczyć możliwość wykorzystania niektórych modeli. PRzy tak duzych danych i wielu kategoriach niemożliwe było zastsowanie one-hot encodingu.

\paragraph{5. Sprawdzenie brakujących wartości}
Po zakończeniu wcześniejszych kroków przeanalizowano brakujące wartości w każdej z kolumn. Wyniki prezentowały się następująco:

\begin{verbatim}
year             0.000000%
make             1.843378%
model            1.860915%
trim             1.906011%
body             2.361263%
transmission    11.695010%
state            0.000000%
condition        2.110553%
odometer         0.016821%
color            0.134035%
interior         0.134035%
seller           0.000000%
mmr              0.000000%
sellingprice     0.000000%
car_age          0.000000%
sale_year        0.000000%
sale_month       0.000000%
sale_hour        0.000000%
sale_minute      0.000000%
sale_weekday     0.000000%
vin_country      0.000000%
\end{verbatim}

\paragraph{6. Uzupełnianie brakujących wartości}
Do imputacji brakujących danych zastosowano trzy metody:
\begin{itemize}
    \item Średnia dla danych numerycznych oraz moda dla danych kategorycznych.
    \item Mediana dla danych numerycznych oraz moda dla danych kategorycznych.
    \item Imputacja metodą k-Nearest Neighbors (\texttt{KNNImputer}) z 3 sąsiadami.
\end{itemize}

\paragraph{7. Skalowanie i standaryzacja danych}
W celu ujednolicenia wartości numerycznych zastosowano dwa podejścia:
\begin{itemize}
    \item Skalowanie Min-Max w zakresie $[0, 1]$
    \item Standaryzacja do średniej 0 i odchylenia standardowego 1
\end{itemize}
Dla każdego z trzech zbiorów (imputacja średnią, medianą, KNN) zastosowano skalowanie i standaryzację, co doprowadziło do utworzenia sześciu różnych zestawów danych:
\begin{verbatim}
mean_min_max.csv
median_min_max.csv
knn_min_max.csv
mean_standard.csv
median_standard.csv
knn_standard.csv
\end{verbatim}

\paragraph{8. Zapis przetworzonych danych}
Wszystkie powyższe zbiory zostały zapisane jako pliki CSV. Zbiory te są gotowe do użycia w modelach predykcyjnych i zapewniają lepszą jakość danych, co przekłada się na większą dokładność modeli.

\subsection{Trening modelu}
\subsubsection{Podział danych na zbiory treningowe i testowe}
Dane podzielono na zbiory treningowe i testowe w proporcji \textbf{80:20}. Proces ten zrealizowano w sposób manualny, zgodnie z zaimplementowanym algorytmem, zamiast korzystania z gotowych funkcji. Podział przebiegał według następujących kroków:
\begin{enumerate}
	\item Dane zostały wczytane i wymieszane przy użyciu funkcji \texttt{shuffle}, co umożliwiło losowe rozłożenie próbek, przy jednoczesnym zapewnieniu powtarzalności wyników dzięki zastosowaniu parametru \texttt{random\_state=42}.
	\item Na podstawie zdefiniowanego indeksu, wyznaczono dwie części zbioru:
	\begin{itemize}
		\item Zbiór treningowy: zawierający pierwsze 80\% obserwacji,
		\item Zbiór testowy: obejmujący pozostałe 20\%.
	\end{itemize}
\end{enumerate}
Podziału dokonano niezależnie dla każdego z sześciu przetworzonych zbiorów danych:

\begin{itemize}
	\item \texttt{mean\_min\_max},
	\item \texttt{median\_min\_max},
	\item \texttt{knn\_min\_max},
	\item \texttt{mean\_standard},
	\item \texttt{median\_standard},
	\item \texttt{knn\_standard}.
\end{itemize}
Każdy zbiór danych zawierał \textbf{21 cech} oraz około \textbf{409,778 obserwacji} w zbiorze treningowym i \textbf{102,445 obserwacji} w zbiorze testowym.

\subsubsection{Proces treningu modeli}
\begin{enumerate}
	\item \textbf{Przygotowanie danych:} Zmienną objaśnianą była \texttt{sellingprice}, natomiast zmiennymi objaśniającymi pozostałe kolumny w zbiorze danych.
	\item \textbf{Trenowanie modeli:} Modele trenowano za pomocą metody \texttt{fit()} na zbiorze treningowym, a predykcje generowano na zbiorze testowym za pomocą \texttt{predict()}.
	\item \textbf{Ocena wyników:} Modele oceniano przy użyciu następujących metryk:
	\begin{itemize}
		\item \textbf{Mean Absolute Error (MAE):} średni błąd bezwzględny,
		\item \textbf{Root Mean Square Error (RMSE):} pierwiastek z błędu średniokwadratowego,
		\item \textbf{\(R^2\):} współczynnik determinacji,
		\item \textbf{Mean Absolute Percentage Error (MAPE):} średni błąd względny, wyrażony w procentach.
	\end{itemize}
	Dodatkowo zarejestrowano czas trenowania każdego modelu.
\end{enumerate}

\newpage
\subsubsection{Modele wykorzystane do analizy}
W eksperymencie wykorzystano następujące modele regresji:
\begin{itemize}
	\item \textbf{Random Forest Regressor},
	\item \textbf{Linear Regression},
	\item \textbf{Histogram-based Gradient Boosting Regressor},
	\item \textbf{XGBoost Regressor},
	\item \textbf{CatBoost Regressor},
	\item \textbf{Voting Regresoor} składający się z \textbf{Linear Regression}, \textbf{Histogram-based Gradient Boosting Regressor}, \textbf{Elastic Net},
	\item \textbf{Stacking Regressor} składający się z \textbf{Linear Regression}, \textbf{Histogram-based Gradient Boosting Regressor}, \textbf{Elastic Net}.
\end{itemize}
Model \textbf{Support Vector Regression (SVR)} został wykluczony ze względu na problemy z wydajnością przy dużych zbiorach danych oraz trudności w przetwarzaniu zmiennych kategorycznych (po zastosowaniu \textit{label encoding}).
W uczeniu zespołowym nie wykorzystano także algorytmu Random Forest ze względu na problemy z wydajnością.

\subsubsection{Wyniki eksperymentów}
Wyniki dla każdego modelu i zbioru danych zostały przedstawione w Tabeli~\ref{tab:results_all}.
\begin{table}[H]
	\centering
\caption{Wyniki dla każdego modelu i zbioru danych.}
\label{tab:results_all}
\begin{tabular}{|c|l|l|c|c|c|c|c|}
	\hline
	\textbf{ID} & \textbf{Zbiór danych} & \textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{\(R^2\)} & \textbf{MAPE} & \textbf{Czas (s)} \\ \hline
	0 & mean\_min\_max & RandomForest & 0.026047 & 0.039195 & 0.968885 & 12.421442 & 377.814284 \\ \hline
	1 & mean\_min\_max & LinearRegression & 0.028074 & 0.042012 & 0.964252 & 14.008762 & 0.405149 \\ \hline
	2 & mean\_min\_max & HistGradientBoostingRegressor & 0.025588 & 0.038294 & 0.970300 & 12.207046 & 11.084602 \\ \hline
	3 & mean\_min\_max & XGBoost & 0.024886 & 0.037412 & 0.971652 & 11.689896 & 3.698308 \\ \hline
	4 & mean\_min\_max & CatBoost & 0.024627 & 0.036911 & 0.972407 & 11.564949 & 70.450874 \\ \hline
	5 & mean\_min\_max & Voting Regressor & 0.065400 & 0.082988 & 0.860514 & 49.292124 & 8.702858 \\ \hline
	6 & mean\_min\_max & Stacking Regressor & 0.025575 & 0.038244 & 0.970378 & 12.154732 & 53.870504 \\ \hline
	7 & median\_min\_max & RandomForest & 0.026043 & 0.039188 & 0.968897 & 12.416976 & 257.189253 \\ \hline
	8 & median\_min\_max & LinearRegression & 0.028074 & 0.042012 & 0.964252 & 14.008762 & 0.354580 \\ \hline
	9 & median\_min\_max & HistGradientBoostingRegressor & 0.025555 & 0.038270 & 0.970337 & 12.196443 & 9.029640 \\ \hline
	10 & median\_min\_max & XGBoost & 0.024845 & 0.037338 & 0.971763 & 11.681776 & 3.241030 \\ \hline
	11 & median\_min\_max & CatBoost & 0.024592 & 0.036882 & 0.972450 & 11.548103 & 68.110814 \\ \hline
	12 & median\_min\_max & Voting Regressor & 0.065387 & 0.082980 & 0.860542 & 49.283282 & 8.520290 \\ \hline
	13 & median\_min\_max & Stacking Regressor & 0.025535 & 0.038212 & 0.970427 & 12.132452 & 50.229682 \\ \hline
	14 & knn\_min\_max & RandomForest & 0.026032 & 0.039191 & 0.968891 & 12.420795 & 255.567970 \\ \hline
	15 & knn\_min\_max & LinearRegression & 0.028098 & 0.042036 & 0.964211 & 14.004907 & 0.364529 \\ \hline
	16 & knn\_min\_max & HistGradientBoostingRegressor & 0.025589 & 0.038308 & 0.970279 & 12.201167 & 10.394278 \\ \hline
	17 & knn\_min\_max & XGBoost & 0.024855 & 0.037345 & 0.971753 & 11.641967 & 3.362012 \\ \hline
	18 & knn\_min\_max & CatBoost & 0.024622 & 0.036968 & 0.972321 & 11.566808 & 74.796223 \\ \hline
	19 & knn\_min\_max & Voting Regressor & 0.065349 & 0.082936 & 0.860688 & 49.191944 & 11.566812 \\ \hline
	20 & knn\_min\_max & Stacking Regressor & 0.025573 & 0.038258 & 0.970356 & 12.151640 & 65.034105 \\ \hline
	21 & mean\_standard & RandomForest & 0.117083 & 0.176223 & 0.968908 & 78.383993 & 260.279401 \\ \hline
	22 & mean\_standard & LinearRegression & 0.126269 & 0.188958 & 0.964252 & 85.846146 & 0.339964 \\ \hline
	23 & mean\_standard & HistGradientBoostingRegressor & 0.115087 & 0.172232 & 0.970300 & 78.263048 & 10.777526 \\ \hline
	24 & mean\_standard & XGBoost & 0.111790 & 0.167926 & 0.971767 & 75.951406 & 3.118486 \\ \hline
	25 & mean\_standard & CatBoost & 0.110740 & 0.166021 & 0.972404 & 75.029054 & 73.516604 \\ \hline
	26 & mean\_standard & Voting Regressor & 0.220150 & 0.286456 & 0.917844 & 74.406331 & 10.642051 \\ \hline
	27 & mean\_standard & Stacking Regressor & 0.115055 & 0.172009 & 0.970377 & 78.510305 & 59.476473 \\ \hline
	28 & median\_standard & RandomForest & 0.117069 & 0.176152 & 0.968933 & 78.477672 & 243.033475 \\ \hline
	29 & median\_standard & LinearRegression & 0.126269 & 0.188958 & 0.964252 & 85.846146 & 0.304093 \\ \hline
	30 & median\_standard & HistGradientBoostingRegressor & 0.114941 & 0.172125 & 0.970337 & 78.244558 & 9.704722 \\ \hline
	31 & median\_standard & XGBoost & 0.111745 & 0.167936 & 0.971763 & 75.989966 & 3.438448 \\ \hline
	32 & median\_standard & CatBoost & 0.110640 & 0.166014 & 0.972406 & 74.672273 & 68.956737 \\ \hline
	33 & median\_standard & Voting Regressor & 0.220089 & 0.286425 & 0.917862 & 74.362599 & 8.220672 \\ \hline
	34 & median\_standard & Stacking Regressor & 0.114877 & 0.171866 & 0.970427 & 78.481290 & 51.174468 \\ \hline
	35 & knn\_standard & RandomForest & 0.117061 & 0.176270 & 0.968891 & 78.289942 & 247.432862 \\ \hline
	36 & knn\_standard & LinearRegression & 0.126377 & 0.189066 & 0.964211 & 85.749496 & 1.059998 \\ \hline
	37 & knn\_standard & HistGradientBoostingRegressor & 0.115077 & 0.172158 & 0.970326 & 78.214080 & 11.462982 \\ \hline
	38 & knn\_standard & XGBoost & 0.111789 & 0.167967 & 0.971753 & 76.288017 & 2.724514 \\ \hline
	39 & knn\_standard & CatBoost & 0.110701 & 0.166215 & 0.972339 & 74.427258 & 49.833858 \\ \hline
	40 & knn\_standard & Voting Regressor & 0.220160 & 0.286490 & 0.917813 & 74.165903 & 8.574647 \\ \hline
	41 & knn\_standard & Stacking Regressor & 0.115046 & 0.172042 & 0.970355 & 78.510303 & 66.176187 \\ \hline
\end{tabular}
\end{table}

\newpage
Kluczowe spostrzeżenia:
\begin{itemize}
	\item \textbf{CatBoost} osiągnął najlepsze wyniki na większości zbiorów danych, minimalizując wartości MAE i RMSE, a także osiągając najwyższy \(R^2\).
	\item \textbf{XGBoost} uzyskał porównywalne wyniki, ale z nieco krótszym czasem trenowania.
	\item \textbf{Random Forest} wymagał znacznie więcej czasu obliczeniowego, co czyniło go mniej wydajnym w porównaniu do gradientowego boostingu.
	\item \textbf{Linear Regression} była najszybsza, ale jej dokładność pozostawiała wiele do życzenia w porównaniu do bardziej zaawansowanych modeli.
\end{itemize}

\begin{table}[H]
	\centering
	\caption{Wyniki dla najlepszych modeli na różnych zbiorach danych.}
	\label{tab:results}
	\begin{tabular}{|l|l|c|c|c|c|c|}
		\hline
		\textbf{Zbiór danych} & \textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{\(R^2\)} & \textbf{MAPE} & \textbf{Czas (s)} \\ \hline
		median\_min\_max      & CatBoost       & 0.024592     & 0.036882      & 0.97245         & 11.548        & 68.110814            \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Porównanie wyników na wykresach}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/mea.png}
	\caption{Porównanie MAE dla różnych modeli i zbiorów danych.}
	\label{fig:mae}
\end{figure}

Na wykresie (Rys.~\ref{fig:mae}) przedstawiono wartości MAE dla wszystkich modeli na sześciu zbiorach danych. Modele \textbf{CatBoost} i \textbf{XGBoost} osiągnęły najlepsze wyniki, szczególnie na zbiorach przetworzonych za pomocą \texttt{min-max scaling}. \textbf{Voiting Regressor} uzyskała najwyższe wartości MAE, co wskazuje na jej niższą skuteczność w porównaniu do innych modeli.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/czasy_rozne_modele.png}
	\caption{Czas trenowania dla różnych modeli i zbiorów danych.}
	\label{fig:time}
\end{figure}

Na wykresie (Rys.~\ref{fig:time}) zaprezentowano czas trenowania poszczególnych modeli na różnych zbiorach danych. Modele \textbf{Linear Regression} i \textbf{XGBoost} charakteryzowały się najkrótszym czasem trenowania (zaledwie kilka sekund na zbiór danych), co czyni je wyjątkowo wydajnymi. Natomiast \textbf{Random Forest} wymagał znacznie więcej czasu obliczeniowego, co może stanowić problem w przypadku dużych zbiorów danych. Modele uczenia zespołowego także wymagały więcej czasu niż modele podstawowe. 

\newpage
\begin{landscape}
\subsubsection{Walidacja krzyżowa}
\begin{table}[H]
	\centering
	\caption{Wyniki cross-walidacji.}
	\label{tab:results_cross}
	\begin{tabular}{|c|l|l|c|c|c|c|c|}
	
	\textbf{ID} & \textbf{Dataset} & \textbf{Model} & \textbf{Manual MAE} & \textbf{Sklearn MAE} & \textbf{Manual RMSE} & \textbf{Sklearn RMSE} & \textbf{\(R^2\)} \\ 
	\hline
0 & mean\_min\_max & LinearRegression & 0.028125 & 0.028125 & 0.042104 & 0.042104 & 0.964139 \\ \hline
1 & mean\_min\_max & XGBoost & 0.024906 & 0.024907 & 0.037639 & 0.037616 & 0.971340 \\ \hline
2 & mean\_min\_max & CatBoost & 0.024669 & 0.024676 & 0.037222 & 0.037232 & 0.971972 \\ \hline
3 & mean\_min\_max & HistGradientBoosting & 0.025667 & 0.025650 & 0.038547 & 0.038553 & 0.969942 \\ \hline
4 & mean\_min\_max & Voting Regressor & 0.065545 & 0.065536 & 0.083119 & 0.083114 & 0.860243 \\ \hline
5 & mean\_min\_max & Stacking Regressor & 0.025653 & 0.025713 & 0.038502 & 0.038597 & 0.970012 \\ \hline
6 & median\_min\_max & LinearRegression & 0.028125 & 0.028125 & 0.042104 & 0.042104 & 0.964139 \\ \hline
7 & median\_min\_max & XGBoost & 0.024905 & 0.024896 & 0.037649 & 0.037621 & 0.971326 \\ \hline
8 & median\_min\_max & CatBoost & 0.024665 & 0.024675 & 0.037220 & 0.037219 & 0.971975 \\ \hline
9 & median\_min\_max & HistGradientBoosting & 0.025663 & 0.025659 & 0.038550 & 0.038560 & 0.969937 \\ \hline
10 & median\_min\_max & Voting Regressor & 0.065539 & 0.065532 & 0.083112 & 0.083109 & 0.860266 \\ \hline
11 & median\_min\_max & Stacking Regressor & 0.025650 & 0.025727 & 0.038506 & 0.038606 & 0.970005 \\ \hline
12 & knn\_min\_max & LinearRegression & 0.028148 & 0.028148 & 0.042127 & 0.042127 & 0.964100 \\ \hline
13 & knn\_min\_max & XGBoost & 0.024895 & 0.024905 & 0.037604 & 0.037613 & 0.971394 \\ \hline
14 & knn\_min\_max & CatBoost & 0.024681 & 0.024678 & 0.037226 & 0.037219 & 0.971966 \\ \hline
15 & knn\_min\_max & HistGradientBoosting & 0.025663 & 0.025663 & 0.038544 & 0.038555 & 0.969947 \\ \hline
16 & knn\_min\_max & Voting Regressor & 0.065494 & 0.065490 & 0.083064 & 0.083063 & 0.860425 \\ \hline
17 & knn\_min\_max & Stacking Regressor & 0.025647 & 0.025705 & 0.038498 & 0.038563 & 0.970019 \\ \hline
18 & mean\_standard & LinearRegression & 0.126496 & 0.126496 & 0.189368 & 0.189368 & 0.964139 \\ \hline
19 & mean\_standard & XGBoost & 0.111978 & 0.112011 & 0.169199 & 0.169189 & 0.971371 \\ \hline
20 & mean\_standard & CatBoost & 0.110914 & 0.111003 & 0.167338 & 0.167466 & 0.971996 \\ \hline
21 & mean\_standard & HistGradientBoosting & 0.115442 & 0.115366 & 0.173369 & 0.173398 & 0.969942 \\ \hline
22 & mean\_standard & Voting Regressor & 0.220838 & 0.220809 & 0.287181 & 0.287173 & 0.917526 \\ \hline
23 & mean\_standard & Stacking Regressor & 0.115399 & 0.115741 & 0.173163 & 0.173536 & 0.970014 \\ \hline
24 & median\_standard & LinearRegression & 0.126496 & 0.126496 & 0.189368 & 0.189368 & 0.964139 \\ \hline
25 & median\_standard & XGBoost & 0.112033 & 0.112007 & 0.169323 & 0.169222 & 0.971329 \\ \hline
26 & median\_standard & CatBoost & 0.110947 & 0.110959 & 0.167407 & 0.167389 & 0.971973 \\ \hline
27 & median\_standard & HistGradientBoosting & 0.115425 & 0.115407 & 0.173386 & 0.173428 & 0.969937 \\ \hline
28 & median\_standard & Voting Regressor & 0.220815 & 0.220783 & 0.287153 & 0.287145 & 0.917542 \\ \hline
29 & median\_standard & Stacking Regressor & 0.115386 & 0.115801 & 0.173185 & 0.173577 & 0.970006 \\ \hline
30 & knn\_standard & LinearRegression & 0.126601 & 0.126601 & 0.189472 & 0.189472 & 0.964100 \\ \hline
31 & knn\_standard & XGBoost & 0.111979 & 0.112037 & 0.169150 & 0.169213 & 0.971387 \\ \hline
32 & knn\_standard & CatBoost & 0.110999 & 0.110999 & 0.167440 & 0.167424 & 0.971962 \\ \hline
33 & knn\_standard & HistGradientBoosting & 0.115423 & 0.115400 & 0.173356 & 0.173384 & 0.969947 \\ \hline
34 & knn\_standard & Voting Regressor & 0.220862 & 0.220841 & 0.287228 & 0.287216 & 0.917500 \\ \hline
35 & knn\_standard & Stacking Regressor & 0.115369 & 0.115763 & 0.173144 & 0.173561 & 0.970020 \\ \hline
\end{tabular}
\end{table}
\end{landscape}

\newpage
\subsubsection{Wnioski}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/porownanie_Błąd_MAE_dla_zbioru_mean_min_max.png}
	\caption{Porównanie błędu MEA dla zbioru mean\_max\_min.}
	\label{fig:mea2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/porownanie_Błąd_MAE_dla_zbioru_mean_standard.png}
	\caption{Porównanie błędu MEA dla zbioru mean\_standard.}
	\label{fig:mea3}
\end{figure}

\newpage
\begin{itemize}
	\item XGBoost i CatBoost oraz HistGradientBoosting wykazują najlepsze wyniki w MAE, RMSE oraz \(R^2\), co sugeruje, że są one najbardziej odpowiednie dla analizowanych zbiorów danych.
	\item Modele z normalizacją (\_min\_max) działają generalnie lepiej niż te z użyciem standaryzacji (\_standard), co może wynikać z charakteru danych.
	\item Voting Regressor osiąga najsłabsze wyniki w każdym scenariuszu, co może sugerować, że średnia z modeli bazowych nie jest optymalna dla tych danych.
	\item Różnice między wynikami obliczonymi manualnie i za pomocą Sklearn są minimalne, co potwierdza poprawność implementacji obliczeń manualnych.
\end{itemize}

\subsubsection{Wnioski z analizy}

\begin{itemize}
\item \textbf{CatBoost} i \textbf{XGBoost} oraz \textbf{ HistGradientBoostingRegressor} okazały się najskuteczniejszymi modelami, osiągając zarówno wysoką dokładność predykcji (niski MAE i RMSE), jak i akceptowalny czas trenowania. Wskazuje to na ich przydatność w zadaniach związanych z predykcją cen samochodów.
\item Modele \textbf{Linear Regression} oraz \textbf{Random Forest} miały znaczące ograniczenia - pierwszy charakteryzował się niską dokładnością, a drugi wymagał znacznie dłuższego czasu obliczeniowego, co czyni go mniej praktycznym w zastosowaniach na dużych zbiorach danych.
\item Analiza wskazała, że przetwarzanie danych za pomocą \textbf{min-max scaling} było bardziej efektywne niż standaryzacja, co podkreśla znaczenie odpowiedniego przygotowania danych w procesie modelowania.
\end{itemize}

\newpage
\subsection{Optymalizacja}
W wyniku otrzymanych rezultatów do optymalizacji zostanie wykorzystany model \textbf{Histogram-based Gradient Boosting Regressor} ze względu na czas działania i skuteczność. Wykorzystany zostanie zbiór \textbf{mean\_min\_max.csv}.

\section{Podsumowanie}

\bibliographystyle{plain}
\bibliography{bibliography.bib} 

\end{document}